---
title: "k近邻"
catagories:
  - Machine Learning
mathjax: true
---
# k近邻法

针对分类问题

**没有显式的学习过程，其训练集即为其“模型”**

## k近邻算法

输入：训练数据集
$$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$$
其中，$x_i\in\mathcal{X}\subseteq R^n$为实例的特征向量，$y_i \in \mathcal{Y} =\{c_1,c_2,...,c_K\}$为实例类别，$i=1,2,...N$;

​输出：实例$x$所属类$y$
1. 根据给定的**距离度量**，在训练集$T$中找出与$x$距离最近的$k$个点($k$近邻)，包含这$k$个点的$x$的邻域记作$N_k(x)$
2. 在$N_k(x)$中根据分类决策规则，(最典型的 **多数表决**)决定$x$的类别$y$：
$$y=arg \max \limits_{c_j} \sum \limits_{x_i\in N_k(x)} I(y_i=c_j),i=1,2,...,N; j=1,2,...K$$
其中，$I$为指示函数，
$$ I=\begin {cases}1,y_i=c_j \\ 0,y_i\ne c_j \end{cases}{} $$
**没有类似感知机中的参数的学习过程，直接将输入的训练数据作为模型**

## k近邻模型

### 距离度量

不同的距离度量算出的距离不同，其确定的最近邻点也不同

> 距离度量：对于一个度量空间，其距离度量可以有多种
> 一般为欧式距离
> $$d(x_i,x_j)=\sqrt{\sum \limits_{l=1} \limits^{n} |x_i^{(l)}-x_j^{(l)}|^2}$$
> 更一般的，$L_p$距离
> $$L_p(x_i,x_j)=\sqrt[p]{\sum \limits_{l=1} \limits^{n} |x_i^{(l)}-x_j^{(l)}|^p}, p\geqslant1$$
> ![不同p值下与零点距离为1的点集]('/uploads/2017-07-04-k-NN/L_p%20distance.png')
> 邻域：包含该点的开球，一般以该点为中心

### k值的选择

若$k$较小，即相当于用一个较小的邻域中的训练实例进行预测

* *近似误差*将减小，只有与实例较为接近的训练实例才会对预测结果起作用
* *估计误差*将增大，预测结果对近邻的实例点更敏感。若附近的点恰好为噪声，预测便出错
* $k$值减小将使整体模型变得复杂，易**过拟合**
* $k=1$，成为*最近邻算法*

若$k$较大，即相当于用一个较大的邻域中的训练实例进行预测

* 估计误差减小，近似误差增大。较远的数据点也将影响预测
* 整体模型变得简单
* $k$过大($N$)，模型过于简单，忽略训练集中大量有用信息，不可取

如何确定$k$值？**交叉验证**

> 如果样本充足，比较简单的方法是将数据集分为三部分：训练集，验证集，测试集
> 用训练集训练多个不同复杂度的模型，用验证集选择最佳模型，用测试集进行评估
> **大部分应用中数据不充足，交叉验证**
> 即分为训练集与测试集，重复使用数据，反复训练
> * 简单交叉验证
>   分为训练集与测试集，用训练集训练出多个模型，用测试集评价，选出误差最小的模型
> * S折交叉验证
>   将数据切分为S个等大子集，用S-1个子集数据进行训练，剩下一个作为测试集。选取训练出的模型中误差最小的那一个
>   * S=N时，称为留一交叉验证

### 分类决策规则

往往是多数表决
若记分类的损失函数为0-1损失函数

$$L(Y,f(X))=\begin{cases}1,Y\ne f(X) \\0,Y=f(X) \end{cases}$$

那么将有：多数表决等价于经验风险最小化（即误分类率最小化）

>  误分类率：$$\frac{1}{k}\sum\limits_{x_i\in N_k(x)}I(y_i\neq c_j)=1-\frac{1}{k}\sum\limits_{x_i\in N_k(x)}I(y_i= c_j)$$

## k近邻法的实现：kd树

最简单的$k$ 近邻法的实现：线性扫描，遍历整个训练集计算距离

 $kd$ 树：对$k$维空间中的实例点进行储存，以进行快速检索的数据结构

* 二叉树，构造时不断用超平面切分空间，构成一系列超矩形区域。树的每一个结点对应一个超矩形区域

**这里的$k$是指$k$维空间，与$k$近邻法的$k$意义不同**

### 算法：构造kd树

​	输入：$k$维空间数据集$$T=\{x_1,x_2,...,x_N\}$$

其中$$x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(k)})^T,i=1,2,...,N$$

​	输出：$kd$树

​	（1）开始：根节点：包含$$T$$的$k$维空间的超矩形区域

​	$$x^{(1)}$$为坐标轴，以$$T$$中所有实例的$$x^{(1)}$$坐标的中位数作为切分点，将根节点的超矩形区域分为两个子区域。

​	在这个根结点中，保存所有落在该切分超平面上的实例点。

​	（2）重复：对每个深度为$$j$$的结点，记$$l=j(modk)+1$$，选择$$x^{(l)}$$为切分坐标轴，以**该结点的超矩形区域**中所有实例的$$x^{(l)}$$坐标的中位数作为切分点，将该节点的超矩形区域分为两个子区域。

​	在这个结点中，保存所有落在该切分超平面上的实例点。

​	（3）当两个子区域中均无实例存在时停止。$kd$树构造完毕

#### 简单的例子

$$T=\{(2,3)^T,(5,4)^T,(9,6)^T,(4,7)^T,(8,1)^T,(7,2)^T\}$$

![Tree](/uploads/2017-07-04-k-NN/tree.png)

### 搜索kd树

$kd$树的每个结点的内容

* 一个超平面，以及其上面的实例点，被保存于该结点中
* 一个超矩形区域

#### 最近邻搜索算法

​	输入：$kd$树，目标点$x$

​	输出：$x$的最近邻

​	（1）找出包含目标点的叶结点：从根结点开始，递归地向下访问：如果$x$当前维的坐标小于切分点的坐标，进入左子结点，否则进入右子结点。直到访问到叶结点为止。

​	（2）记该叶结点为“当前最近点”

​	（3）从当前最近点开始，递归地往根结点回退，在每个结点进行如下操作：

​	（a）若当前结点中**所保存的实例点**比当前最近点的距离短，以该实例点为当前最近点

​	（b）检查当前结点的父结点的另一子结点对应的超矩形区域，是否与当前最短距离为半径、$x$为球心形成的超球体相交。

​		若相交，则*该子结点对应的超矩形区域中的实例点可能存在比当前最近点更近的点*，移至这一子结点，以其为根结点，递归地进行最近邻搜索（step（1））

​		若不相交，向根结点方向回退

​	（4）回退到根结点时搜索结束。最后的当前最近点即为$x$的最近邻点

#### 示例

![kd tree](/uploads/2017-07-04-k-NN/ex.png)

#### 如何改进以寻找k近邻？

将当前最近点改为一个长为$k$的链表，初始化为无限远点。

每次比较，以链表末尾（当前$k$近邻中最远点）作为当前最近点来比较。每次更新链表时插入新结点，移除末结点